# Parameter Overview

<br>

# Stochastic Gradient Descent (SGD)

<br>

# Hyperparameters

* Parameters that can be adjusted at the time of training (*Learning rate*, *Batch size*, *EPOCH*, *Regularization and regularization rate*, *Optimizer*, *etc.*)

<br>

## Batch Size

* The total number of example to calculate Gradient in single iterations

<br>

### Mini-Batch SGD

## EPOD

* The full training pass over the entire training set