# Distributed Training Overview

* Using the right hardware configuration can dramatically reduce training time and provide faster iteration to reach your modeling goals

<br>

* MirroredStrategy and MultiWorkerMirroredStrategy are both synchronized and all nodes are dependent on each other and therefor not fault tolerant