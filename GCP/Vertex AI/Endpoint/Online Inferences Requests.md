# Online Inference Request Overview

An online inference request is a synchronous prediction call made to a ML model that has already been deployed to a Vertex AI endpoint

* Can respond with predictions quickly
* This is possible since the already deployed model is hosted with allocated compute resources